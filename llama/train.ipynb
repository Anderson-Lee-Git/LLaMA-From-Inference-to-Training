{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLaMA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../data/'\n",
    "# TEST_PATH = '/home/patrick/Documents/CSE_599/HW/2/testset/test.jsonl'\n",
    "# VAL_PATH = '/home/patrick/Documents/CSE_599/HW/2/valset/val.jsonl'\n",
    "MODEL_PATH = '/Users/anderson/Desktop/Project/LLaMA-From-Inference-to-Training/' #folder with generation.py, model.py, and tokenizer.py\n",
    "TRAINED_SPM_PATH = './tokenizer.model' #downloaded from Ed post"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Make sure INGESTED_SAMPLE_CNT % MAX_BSZ == 0; assymetric batches break something somewhere\n",
    "# MAX_SEQ_LEN = 2048\n",
    "MAX_SEQ_LEN = 256\n",
    "INGESTED_SAMPLE_CNT = 8\n",
    "MAX_BSZ = 16\n",
    "MINI_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(MODEL_PATH)\n",
    "\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "from generation import LLaMA\n",
    "from llama.model_train import ModelArgs, Transformer #ctrl+f and comment out cuda, all else same\n",
    "#from model import ModelArgs, Transformer #use this one if you have NVIDIA GPU\n",
    "from tokenizer import Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tokenizer = Tokenizer(TRAINED_SPM_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np \n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, random_mask=False, n_mask=1):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.random_mask = random_mask\n",
    "        self.n_mask = n_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoded_text = self.tokenizer.encode(text, bos=False, eos=False)\n",
    "\n",
    "        if not self.random_mask: \n",
    "            #Truncate the sequence to max_seq_len if it's too long\n",
    "            if len(encoded_text) > MAX_SEQ_LEN - 1:  #We subtract 2 to account for the BOS and EOS tokens\\\n",
    "                                                    #small modification: -1 instead\n",
    "                encoded_text = encoded_text[:MAX_SEQ_LEN - 1]           \n",
    "            return {\n",
    "                'input_ids': torch.tensor([self.tokenizer.bos_id] + encoded_text[:-1], dtype=torch.long),\n",
    "                'target_ids': torch.tensor(encoded_text[1:] + [self.tokenizer.eos_id], dtype=torch.long)\n",
    "            }\n",
    "        else: \n",
    "            if len(encoded_text) > MAX_SEQ_LEN - 2:\n",
    "                encoded_text = encoded_text[:MAX_SEQ_LEN - 2]\n",
    "            return {\n",
    "                'input_ids': torch.tensor([self.tokenizer.bos_id] + self.mask(encoded_text) + [self.tokenizer.eos_id], dtype=torch.long),\n",
    "                'target_ids': torch.tensor([self.tokenizer.bos_id] + encoded_text + [self.tokenizer.eos_id], dtype=torch.long)\n",
    "            }\n",
    "    \n",
    "    def mask(self, encoded_text): \n",
    "        idxs = np.random.choice(range(len(encoded_text)), size=self.n_mask)\n",
    "        encoded_text[idxs] = self.tokenizer.pad_id\n",
    "        return encoded_text\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    target_ids = [item['target_ids'] for item in batch]\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_id)\n",
    "    target_ids = pad_sequence(target_ids, batch_first=True, padding_value=tokenizer.pad_id)\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'target_ids': target_ids\n",
    "    }\n",
    "\n",
    "def train_test_split(file_path, train_n, valid_n, test_n): \n",
    "    with open(file_path, \"r\") as file: \n",
    "        train_texts = []\n",
    "        val_texts = []\n",
    "        test_texts = []\n",
    "        for idx, line in enumerate(file.readlines()): \n",
    "            if idx < train_n: \n",
    "                train_texts.append(line)\n",
    "            elif idx < train_n + valid_n: \n",
    "                val_texts.append(line)\n",
    "            elif idx < train_n + valid_n + test_n: \n",
    "                test_texts.append(line) \n",
    "            else: \n",
    "                break \n",
    "    return train_texts, val_texts, test_texts\n",
    "\n",
    "train_texts, val_texts, test_texts = train_test_split(\"../data/11_10000_entries.txt\", 25000, 100, 2000)\n",
    "\n",
    "train_dataset = TextDataset(train_texts, tokenizer)\n",
    "val_dataset = TextDataset(val_texts, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=MAX_BSZ, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=MAX_BSZ, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type: <class 'list'>\n",
      "Length: 25000\n",
      "First element type: <class 'str'>\n",
      "DATALOADER OVERVIEW\n",
      "Number of Batches: 1563\n",
      "Batch Size: 16\n",
      "\n",
      "First batch overview:\n",
      "Keys:  dict_keys(['input_ids', 'target_ids'])\n",
      "Shape of input_ids tensor: torch.Size([16, 171])\n",
      "Shape of target_ids tensor: torch.Size([16, 171])\n",
      "\n",
      "Dataset overview:\n",
      "Length of Dataset:  25000\n",
      "Example item:  {'input_ids': tensor([    1, 29301,   310,  6615,  1230, 18530,  4603,   472,   278,  4768,\n",
      "         5996,  1889,  3233, 29889]), 'target_ids': tensor([  310,  6615,  1230, 18530,  4603,   472,   278,  4768,  5996,  1889,\n",
      "         3233, 29889,    13,     2])}\n"
     ]
    }
   ],
   "source": [
    "def examine_data(data):\n",
    "    '''debugging func'''\n",
    "    print('Input type: {}'.format(type(data)))\n",
    "    print('Length: {}'.format(len(data)))\n",
    "    print('First element type: {}'.format(type(data[0])))\n",
    "    if type(data[0])==dict:\n",
    "        print('Keys: {}'.format(data[0].keys()))\n",
    "    return\n",
    "\n",
    "#debugging tools\n",
    "def examine_tensor(tensor):\n",
    "    '''debugging function'''\n",
    "    print('TENSOR OVERVIEW\\n'\n",
    "          'Type: {}\\n'\n",
    "          'Data Type: {}\\n'\n",
    "          'Shape: {}\\n'\n",
    "          'Number of Dimensions: {}\\n'\n",
    "          'Device: {}\\n'\n",
    "          'Requires Grad: {}\\n'\n",
    "          'Gradient: {}\\n'.format(tensor.type(), tensor.dtype, tensor.shape, tensor.ndim, tensor.device,\\\n",
    "                                  tensor.requires_grad, tensor.grad))\n",
    "    return\n",
    "\n",
    "def flag(msg='unspecified'):\n",
    "    print('FLAG: {}'.format(msg))\n",
    "    return\n",
    "\n",
    "def loop_summary(titles:tuple, tensors:tuple):\n",
    "    for i in range(len(titles)):\n",
    "        flag(titles[i])\n",
    "        examine_tensor(tensors[i])\n",
    "    return\n",
    "\n",
    "def examine_dataloader(dataloader):\n",
    "    '''debugging function'''\n",
    "    print('DATALOADER OVERVIEW\\n'\n",
    "          'Number of Batches: {}\\n'\n",
    "          'Batch Size: {}\\n'.format(len(dataloader), dataloader.batch_size))\n",
    "\n",
    "    # Examine the first batch in the dataloader\n",
    "    first_batch = next(iter(dataloader))\n",
    "    print('First batch overview:')\n",
    "    print('Keys: ', first_batch.keys())\n",
    "    \n",
    "    for key in first_batch.keys():\n",
    "        print('Shape of {} tensor: {}'.format(key, first_batch[key].shape))\n",
    "    \n",
    "    # Examine the dataset\n",
    "    print('\\nDataset overview:')\n",
    "    print('Length of Dataset: ', len(dataloader.dataset))\n",
    "\n",
    "    # Try getting an item from the dataset\n",
    "    try:\n",
    "        print('Example item: ', dataloader.dataset[0])\n",
    "    except Exception as e:\n",
    "        print('Could not retrieve item from dataset: ', str(e))\n",
    "    return\n",
    "\n",
    "examine_data(train_texts)\n",
    "examine_dataloader(train_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure environment for CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: RANK=0\n",
      "env: WORLD_SIZE=1\n",
      "env: MASTER_ADDR=localhost\n",
      "env: MASTER_PORT=0\n",
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    }
   ],
   "source": [
    "import torch.distributed as dist\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "\n",
    "%env RANK=0\n",
    "%env WORLD_SIZE=1\n",
    "%env MASTER_ADDR=localhost\n",
    "%env MASTER_PORT=0\n",
    "\n",
    "torch.distributed.init_process_group(backend='gloo')\n",
    "fs_init.initialize_model_parallel(1) #1 worker\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArgs(\n",
    "    dim=512,\n",
    "    n_layers=8,\n",
    "    n_heads=8,\n",
    "    vocab_size=tokenizer.n_words,\n",
    "    multiple_of=256,\n",
    "    norm_eps=1e-5,\n",
    "    max_batch_size=MAX_BSZ,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "mini_args = ModelArgs(\n",
    "    dim=256,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    vocab_size=tokenizer.n_words,\n",
    "    multiple_of=256,\n",
    "    norm_eps=1e-5,\n",
    "    max_batch_size=MAX_BSZ, #only works for 32; no idea why\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "if MINI_MODEL: #global var (2nd cell)\n",
    "    model = Transformer(mini_args)\n",
    "else:\n",
    "    model = Transformer(model_args)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "loss_func = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)  # ignores padding token for loss calculation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss = 10.378463745117188\n",
      "validation loss = 10.16610676901681\n",
      "training loss = 10.244503021240234\n",
      "validation loss = 9.772968973432269\n",
      "training loss = 9.651533126831055\n",
      "validation loss = 9.37235232761928\n",
      "training loss = 9.341501235961914\n",
      "validation loss = 9.15149620601109\n",
      "training loss = 8.922528266906738\n",
      "validation loss = 8.81408200945173\n",
      "training loss = 8.736300468444824\n",
      "validation loss = 8.623375756399971\n",
      "training loss = 8.116381645202637\n",
      "validation loss = 8.029822962624687\n",
      "training loss = 8.359764099121094\n",
      "validation loss = 8.087396689823695\n",
      "training loss = 8.035664558410645\n",
      "validation loss = 7.925850868225098\n",
      "training loss = 7.9518723487854\n",
      "validation loss = 8.052045277186803\n",
      "training loss = 8.038518905639648\n",
      "validation loss = 8.10660185132708\n",
      "training loss = 7.108040809631348\n",
      "validation loss = 8.198188032422747\n",
      "training loss = 8.326313972473145\n",
      "validation loss = 8.163196427481514\n",
      "training loss = 8.226258277893066\n",
      "validation loss = 7.953767367771694\n",
      "training loss = 7.148224830627441\n",
      "validation loss = 7.990347862243652\n",
      "training loss = 8.321152687072754\n",
      "validation loss = 8.088367666516985\n",
      "training loss = 8.444022178649902\n",
      "validation loss = 7.957220213753836\n",
      "training loss = 7.965025424957275\n",
      "validation loss = 8.087754658290319\n",
      "training loss = 7.82823371887207\n",
      "validation loss = 7.967111723763602\n",
      "training loss = 7.9097208976745605\n",
      "validation loss = 7.731420925685337\n",
      "training loss = 7.784358501434326\n",
      "validation loss = 7.934819493974958\n",
      "training loss = 8.103777885437012\n",
      "validation loss = 7.846812656947544\n",
      "training loss = 7.592862606048584\n",
      "validation loss = 7.227708339691162\n",
      "training loss = 7.655186176300049\n",
      "validation loss = 7.797662598746164\n",
      "training loss = 8.013861656188965\n",
      "validation loss = 7.608884470803397\n",
      "training loss = 7.939559459686279\n",
      "validation loss = 7.537586620875767\n",
      "training loss = 6.799143314361572\n",
      "validation loss = 7.926901681082589\n",
      "training loss = 8.0333251953125\n",
      "validation loss = 7.85030916758946\n",
      "training loss = 7.6877593994140625\n",
      "validation loss = 7.873010090419224\n",
      "training loss = 7.393226146697998\n",
      "validation loss = 7.9489849635532925\n",
      "training loss = 6.879948616027832\n",
      "validation loss = 8.024796417781285\n",
      "training loss = 7.429222583770752\n",
      "validation loss = 7.964859962463379\n",
      "training loss = 7.692800998687744\n",
      "validation loss = 7.667727266039167\n",
      "training loss = 7.822728157043457\n",
      "validation loss = 7.894917828696115\n",
      "training loss = 7.18207311630249\n",
      "validation loss = 7.890134538922991\n",
      "training loss = 7.428554058074951\n",
      "validation loss = 7.944471154894147\n",
      "training loss = 7.461993217468262\n",
      "validation loss = 7.750460488455636\n",
      "training loss = 7.84749698638916\n",
      "validation loss = 7.902059759412493\n",
      "training loss = 7.782339096069336\n",
      "validation loss = 7.8973678180149625\n",
      "training loss = 7.289596080780029\n",
      "validation loss = 7.432734966278076\n",
      "training loss = 7.035224437713623\n",
      "validation loss = 7.277851377214704\n",
      "training loss = 7.951666831970215\n",
      "validation loss = 7.670485973358154\n",
      "training loss = 6.722356796264648\n",
      "validation loss = 7.74382884161813\n",
      "training loss = 8.287522315979004\n",
      "validation loss = 7.693302290780204\n",
      "training loss = 7.466508388519287\n",
      "validation loss = 7.708421979631696\n",
      "training loss = 8.0050687789917\n",
      "validation loss = 7.612451621464321\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m train_history \u001b[39m=\u001b[39m [] \n\u001b[1;32m     44\u001b[0m val_history \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 45\u001b[0m train_history, val_history \u001b[39m=\u001b[39m train()\n",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     outputs \u001b[39m=\u001b[39m model(val_inputs, start_pos\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     27\u001b[0m     flat_outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, outputs\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39m#(bsz*seq_len) x vocab_size\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     flat_targets \u001b[39m=\u001b[39m val_targets\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m#(bsz*seq_len)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     val_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_func(flat_outputs, flat_targets)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     31\u001b[0m val_loss \u001b[39m=\u001b[39m val_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(val_dataloader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch['input_ids'] #bsz x seq_len \n",
    "        targets = batch['target_ids'] #bsz x seq_len\n",
    "        # examine_tensor(targets)\n",
    "\n",
    "        outputs = model(inputs, start_pos=0) #bsz x sel_len x vocab_size\n",
    "\n",
    "        flat_outputs = outputs.view(-1, outputs.size(-1)) #(bsz*seq_len) x vocab_size\n",
    "\n",
    "        flat_targets = targets.view(-1) #(bsz*seq_len)\n",
    "\n",
    "        loss = loss_func(flat_outputs, flat_targets) #might be incorrect to flatten, idk\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        val_loss = 0\n",
    "\n",
    "        for val_batch in val_dataloader: \n",
    "            val_inputs = val_batch['input_ids']\n",
    "            val_targets = val_batch['target_ids']\n",
    "\n",
    "            outputs = model(val_inputs, start_pos=0)\n",
    "            flat_outputs = outputs.view(-1, outputs.size(-1)) #(bsz*seq_len) x vocab_size\n",
    "            flat_targets = val_targets.view(-1) #(bsz*seq_len)\n",
    "            val_loss += loss_func(flat_outputs, flat_targets).item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_dataloader)\n",
    "        \n",
    "        print(f\"training loss = {loss.item()}\")\n",
    "        print(f\"validation loss = {val_loss}\")\n",
    "        total_loss += loss.item()\n",
    "        train_history.append(loss.item())\n",
    "        val_history.append(val_loss)\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    return train_history, val_history \n",
    "\n",
    "train_history = [] \n",
    "val_history = []\n",
    "train_history, val_history = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m plt(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(train_history)), train_history)\n\u001b[1;32m      4\u001b[0m plt(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(val_history)), val_history)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt(range(len(train_history)), train_history)\n",
    "plt(range(len(val_history)), val_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "dir = './ckpts/test_ckpt'\n",
    "if os.path.exists(dir):\n",
    "    shutil.rmtree(dir)\n",
    "os.makedirs(dir)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, \"ckpts/test_ckpt/model.pth\")\n",
    "\n",
    "json_params = json.dumps({\n",
    "    \"dim\": model_args.dim, \n",
    "    \"n_layers\": model_args.n_layers, \n",
    "    \"n_heads\": model_args.n_heads, \n",
    "    \"vocab_size\": model_args.vocab_size, \n",
    "    \"multiple_of\": model_args.multiple_of, \n",
    "    \"norm_eps\": model_args.norm_eps, \n",
    "}, indent=4)\n",
    "\n",
    "with open(dir + \"/params.json\", \"w\") as outfile:\n",
    "    outfile.write(json_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMATH-Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
