{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLaMA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../data/'\n",
    "# TEST_PATH = '/home/patrick/Documents/CSE_599/HW/2/testset/test.jsonl'\n",
    "# VAL_PATH = '/home/patrick/Documents/CSE_599/HW/2/valset/val.jsonl'\n",
    "MODEL_PATH = '/Users/anderson/Desktop/Project/LLaMA-From-Inference-to-Training/' #folder with generation.py, model.py, and tokenizer.py\n",
    "TRAINED_SPM_PATH = './tokenizer.model' #downloaded from Ed post"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(MODEL_PATH)\n",
    "\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "from generation import LLaMA\n",
    "from llama.model import ModelArgs, Transformer #ctrl+f and comment out cuda, all else same\n",
    "#from model import ModelArgs, Transformer #use this one if you have NVIDIA GPU\n",
    "from tokenizer import Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_list(filepath:str, maxiter:int):\n",
    "    '''ingests JSON into list (with tripwire parameter to prevent computer from crashing)'''\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= maxiter:\n",
    "                break\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        for text in texts:\n",
    "            encodings = tokenizer.encode(text, bos=True, eos=True)\n",
    "\n",
    "            # TODO: Why this? \n",
    "            #takes all but the last token as input and all but the first token as target\n",
    "            self.inputs.append(torch.tensor(encodings[:-1], dtype=torch.long))\n",
    "            self.targets.append(torch.tensor(encodings[1:], dtype=torch.long))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.inputs[idx],\n",
    "                \"target_ids\": self.targets[idx]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    target_ids = [item['target_ids'] for item in batch]\n",
    "    \n",
    "    max_seq_len = 2048 #Truncate sequences\n",
    "    input_ids = [ids[:max_seq_len] for ids in input_ids]\n",
    "    target_ids = [ids[:max_seq_len] for ids in target_ids]\n",
    "    \n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0) #Add padding\n",
    "    target_ids = pad_sequence(target_ids, batch_first=True, padding_value=0)\n",
    "    return {'input_ids': input_ids, 'target_ids': target_ids}\n",
    "\n",
    "# train_data = make_data_list(TRAIN_PATH, 10)\n",
    "# test_data = make_data_list(TEST_PATH, 2)\n",
    "# val_data = make_data_list(VAL_PATH, 1)\n",
    "\n",
    "# def extract_texts(data_list):\n",
    "#     '''gets rid of the metadata'''\n",
    "#     return [item['text'] for item in data_list]\n",
    "\n",
    "def train_test_split(file_path, train_n, valid_n, test_n): \n",
    "    with open(file_path, \"r\") as file: \n",
    "        train_texts = []\n",
    "        val_texts = []\n",
    "        test_texts = []\n",
    "        for idx, line in enumerate(file.readlines()): \n",
    "            if idx < train_n: \n",
    "                train_texts.append(line)\n",
    "            elif idx < valid_n: \n",
    "                val_texts.append(line)\n",
    "            elif idx < test_n: \n",
    "                test_texts.append(line) \n",
    "            else: \n",
    "                break \n",
    "    return train_texts, val_texts, test_texts\n",
    "\n",
    "train_texts, val_texts, test_texts = train_test_split(\"../data/11.txt\", 20, 10, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(TRAINED_SPM_PATH)\n",
    "train_dataset = TextDataset(train_texts, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure environment for CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: RANK=0\n",
      "env: WORLD_SIZE=1\n",
      "env: MASTER_ADDR=localhost\n",
      "env: MASTER_PORT=0\n",
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    }
   ],
   "source": [
    "import torch.distributed as dist\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "\n",
    "%env RANK=0\n",
    "%env WORLD_SIZE=1\n",
    "%env MASTER_ADDR=localhost\n",
    "%env MASTER_PORT=0\n",
    "\n",
    "torch.distributed.init_process_group(backend='gloo')\n",
    "fs_init.initialize_model_parallel(1) #1 worker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ModelArgs, Transformer\n",
    "\n",
    "model_args = ModelArgs(\n",
    "    dim=512,\n",
    "    n_layers=8,\n",
    "    n_heads=8,\n",
    "    vocab_size=tokenizer.n_words,\n",
    "    multiple_of=256,\n",
    "    norm_eps=1e-5,\n",
    "    max_batch_size=32,\n",
    "    max_seq_len=2048\n",
    ")\n",
    "\n",
    "model = Transformer(model_args)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "# TODO: How does the tokenizer have the pad_id? Isn't the padding coming from the collate_fn\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)  #ignores padding token (0) for loss calculation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_tensor(tensor):\n",
    "    '''debugging function'''\n",
    "    print(tensor)\n",
    "    print(\"Type:\", tensor.type())\n",
    "    print(\"Data Type:\", tensor.dtype)\n",
    "    print(\"Shape:\", tensor.shape)\n",
    "    print(\"Size:\", tensor.size())\n",
    "    print(\"Number of Dimensions:\", tensor.ndim)\n",
    "    print(\"Device:\", tensor.device)\n",
    "    print(\"Requires Grad:\", tensor.requires_grad)\n",
    "    print(\"Gradient:\", tensor.grad)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  512,  2058,   310,  ...,     0,     0,     0],\n",
      "        [  739,   338,   553,  ...,     0,     0,     0],\n",
      "        [29301,   310,  6615,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  379,  2121,   517,  ...,     0,     0,     0],\n",
      "        [29871,    13,     2,  ...,     0,     0,     0],\n",
      "        [ 1346,  4806,  1073,  ...,     0,     0,     0]])\n",
      "Type: torch.LongTensor\n",
      "Data Type: torch.int64\n",
      "Shape: torch.Size([20, 342])\n",
      "Size: torch.Size([20, 342])\n",
      "Number of Dimensions: 2\n",
      "Device: cpu\n",
      "Requires Grad: False\n",
      "Gradient: None\n",
      "h shape = torch.Size([20, 342, 512])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (20) to match target batch_size (6840).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch, total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataloader)))\n\u001b[1;32m     21\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 22\u001b[0m train(model, train_dataloader, optimizer, loss_function, num_epochs)\n",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss_function, num_epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, start_pos\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m#forward pass\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# examine_tensor(outputs)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m loss \u001b[39m=\u001b[39m loss_function(outputs\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, outputs\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), target_ids\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     15\u001b[0m loss\u001b[39m.\u001b[39mbackward() \u001b[39m#backward pass\u001b[39;00m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Desktop/Project/LLaMA-From-Inference-to-Training/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Project/LLaMA-From-Inference-to-Training/.conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/Desktop/Project/LLaMA-From-Inference-to-Training/.conda/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (20) to match target batch_size (6840)."
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, optimizer, loss_function, num_epochs):\n",
    "    '''the training loop'''\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()  # reset the gradients\n",
    "            input_ids = batch['input_ids']\n",
    "            target_ids = batch['target_ids']\n",
    "            # examine_tensor(input_ids)\n",
    "            examine_tensor(target_ids)\n",
    "            outputs = model(input_ids, start_pos=0) #forward pass\n",
    "            # examine_tensor(outputs)\n",
    "            loss = loss_function(outputs.view(-1, outputs.size(-1)), target_ids.view(-1))\n",
    "            loss.backward() #backward pass\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(\"Epoch: {}, Loss: {:.4f}\".format(epoch, total_loss / len(dataloader)))\n",
    "        \n",
    "num_epochs = 1\n",
    "train(model, train_dataloader, optimizer, loss_function, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMATH-Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
