{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLaMA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../data/'\n",
    "# TEST_PATH = '/home/patrick/Documents/CSE_599/HW/2/testset/test.jsonl'\n",
    "# VAL_PATH = '/home/patrick/Documents/CSE_599/HW/2/valset/val.jsonl'\n",
    "MODEL_PATH = '/Users/anderson/Desktop/Project/LLaMA-From-Inference-to-Training/' #folder with generation.py, model.py, and tokenizer.py\n",
    "TRAINED_SPM_PATH = './tokenizer.model' #downloaded from Ed post"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(MODEL_PATH)\n",
    "\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "from generation import LLaMA\n",
    "from llama.model import ModelArgs, Transformer #ctrl+f and comment out cuda, all else same\n",
    "#from model import ModelArgs, Transformer #use this one if you have NVIDIA GPU\n",
    "from tokenizer import Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_list(filepath:str, maxiter:int):\n",
    "    '''ingests JSON into list (with tripwire parameter to prevent computer from crashing)'''\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= maxiter:\n",
    "                break\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        for text in texts:\n",
    "            encodings = tokenizer.encode(text, bos=True, eos=True)\n",
    "\n",
    "            # TODO: Why this? \n",
    "            #takes all but the last token as input and all but the first token as target\n",
    "            self.inputs.append(torch.tensor(encodings[:-1], dtype=torch.long))\n",
    "            self.targets.append(torch.tensor(encodings[1:], dtype=torch.long))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.inputs[idx],\n",
    "                \"target_ids\": self.targets[idx]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    target_ids = [item['target_ids'] for item in batch]\n",
    "    \n",
    "    max_seq_len = 2048 #Truncate sequences\n",
    "    input_ids = [ids[:max_seq_len] for ids in input_ids]\n",
    "    target_ids = [ids[:max_seq_len] for ids in target_ids]\n",
    "    \n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0) #Add padding\n",
    "    target_ids = pad_sequence(target_ids, batch_first=True, padding_value=0)\n",
    "    return {'input_ids': input_ids, 'target_ids': target_ids}\n",
    "\n",
    "# train_data = make_data_list(TRAIN_PATH, 10)\n",
    "# test_data = make_data_list(TEST_PATH, 2)\n",
    "# val_data = make_data_list(VAL_PATH, 1)\n",
    "\n",
    "# def extract_texts(data_list):\n",
    "#     '''gets rid of the metadata'''\n",
    "#     return [item['text'] for item in data_list]\n",
    "\n",
    "def train_test_split(file_path, train_n, valid_n, test_n): \n",
    "    with open(file_path, \"r\") as file: \n",
    "        train_texts = []\n",
    "        val_texts = []\n",
    "        test_texts = []\n",
    "        for idx, line in enumerate(file.readlines()): \n",
    "            if idx < train_n: \n",
    "                train_texts.append(line)\n",
    "            elif idx < valid_n: \n",
    "                val_texts.append(line)\n",
    "            elif idx < test_n: \n",
    "                test_texts.append(line) \n",
    "            else: \n",
    "                break \n",
    "    return train_texts, val_texts, test_texts\n",
    "\n",
    "train_texts, val_texts, test_texts = train_test_split(\"../data/11_10000_entries.txt\", 20, 10, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(TRAINED_SPM_PATH)\n",
    "train_dataset = TextDataset(train_texts, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure environment for CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: RANK=0\n",
      "env: WORLD_SIZE=1\n",
      "env: MASTER_ADDR=localhost\n",
      "env: MASTER_PORT=0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "trying to initialize the default process group twice!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39menv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMASTER_ADDR=localhost\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39menv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMASTER_PORT=0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m torch\u001b[39m.\u001b[39;49mdistributed\u001b[39m.\u001b[39;49minit_process_group(backend\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgloo\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m fs_init\u001b[39m.\u001b[39minitialize_model_parallel(\u001b[39m1\u001b[39m) \u001b[39m#1 worker\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Project/LLaMA-From-Inference-to-Training/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:865\u001b[0m, in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    861\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected timeout argument to be of type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdatetime.timedelta\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    862\u001b[0m     )\n\u001b[1;32m    864\u001b[0m \u001b[39mif\u001b[39;00m GroupMember\u001b[39m.\u001b[39mWORLD \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 865\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtrying to initialize the default process group \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtwice!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    867\u001b[0m \u001b[39massert\u001b[39;00m (store \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    868\u001b[0m     init_method \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    869\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mCannot specify both init_method and store.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    871\u001b[0m \u001b[39mif\u001b[39;00m store \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: trying to initialize the default process group twice!"
     ]
    }
   ],
   "source": [
    "import torch.distributed as dist\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "\n",
    "%env RANK=0\n",
    "%env WORLD_SIZE=1\n",
    "%env MASTER_ADDR=localhost\n",
    "%env MASTER_PORT=0\n",
    "\n",
    "torch.distributed.init_process_group(backend='gloo')\n",
    "fs_init.initialize_model_parallel(1) #1 worker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ModelArgs, Transformer\n",
    "\n",
    "model_args = ModelArgs(\n",
    "    dim=512,\n",
    "    n_layers=8,\n",
    "    n_heads=8,\n",
    "    vocab_size=tokenizer.n_words,\n",
    "    multiple_of=256,\n",
    "    norm_eps=1e-5,\n",
    "    max_batch_size=32,\n",
    "    max_seq_len=2048\n",
    ")\n",
    "\n",
    "model = Transformer(model_args)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "# TODO: How does the tokenizer have the pad_id? Isn't the padding coming from the collate_fn\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)  #ignores padding token (0) for loss calculation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_tensor(tensor):\n",
    "    '''debugging function'''\n",
    "    print(tensor)\n",
    "    print(\"Type:\", tensor.type())\n",
    "    print(\"Data Type:\", tensor.dtype)\n",
    "    print(\"Shape:\", tensor.shape)\n",
    "    print(\"Size:\", tensor.size())\n",
    "    print(\"Number of Dimensions:\", tensor.ndim)\n",
    "    print(\"Device:\", tensor.device)\n",
    "    print(\"Requires Grad:\", tensor.requires_grad)\n",
    "    print(\"Gradient:\", tensor.grad)\n",
    "\n",
    "    print(torch.count_nonzero(tensor))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   512,  7786,  ...,     0,     0,     0],\n",
      "        [    1,  2398, 29892,  ...,     0,     0,     0],\n",
      "        [    1, 29871,    13,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    1,  6123,  1100,  ...,     0,     0,     0],\n",
      "        [    1, 29871,    13,  ...,     0,     0,     0],\n",
      "        [    1, 29871,    13,  ...,     0,     0,     0]])\n",
      "h shape = torch.Size([20, 342, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch, total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataloader)))\n\u001b[1;32m     21\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 22\u001b[0m train(model, train_dataloader, optimizer, loss_function, num_epochs)\n",
      "Cell \u001b[0;32mIn[30], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss_function, num_epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# examine_tensor(outputs)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m loss \u001b[39m=\u001b[39m loss_function(outputs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, outputs\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), target_ids\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m---> 15\u001b[0m loss\u001b[39m.\u001b[39;49mbackward() \u001b[39m#backward pass\u001b[39;00m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/Project/LLaMA-From-Inference-to-Training/.conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Project/LLaMA-From-Inference-to-Training/.conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, optimizer, loss_function, num_epochs):\n",
    "    '''the training loop'''\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()  # reset the gradients\n",
    "            input_ids = batch['input_ids']\n",
    "            target_ids = batch['target_ids']\n",
    "            # examine_tensor(input_ids)\n",
    "            # examine_tensor(target_ids)\n",
    "            outputs = model(input_ids, start_pos=0) #forward pass\n",
    "            # examine_tensor(outputs)\n",
    "            loss = loss_function(outputs.view(-1, outputs.size(-1)), target_ids.view(-1))\n",
    "            loss.backward() #backward pass\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(\"Epoch: {}, Loss: {:.4f}\".format(epoch, total_loss / len(dataloader)))\n",
    "        \n",
    "num_epochs = 1\n",
    "train(model, train_dataloader, optimizer, loss_function, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMATH-Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
